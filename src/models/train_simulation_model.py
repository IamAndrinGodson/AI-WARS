"""
Train a model specifically for the Simulation mode.
This ensures the model expects the exact features generated by the application's FeatureEngineer.
"""

import os
import sys
import logging
import random
import numpy as np
import pandas as pd
from datetime import datetime
from pathlib import Path
import joblib
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, f1_score

# Add project root to path
sys.path.insert(0, str(Path(__file__).parent.parent.parent))

from src.features.feature_generator import FeatureEngineer

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def generate_random_ip():
    return f"{random.randint(1, 255)}.{random.randint(0, 255)}.{random.randint(0, 255)}.{random.randint(1, 255)}"

def generate_traffic(num_events, is_attack=False, attack_type=None):
    events = []
    target_ip = "10.0.0.50"
    
    for i in range(num_events):
        event = {
            "timestamp": datetime.now().isoformat(),
            "connection_id": i, # Required by FeatureEngineer for grouping
            "src_ip": generate_random_ip(),
            "dst_ip": target_ip,
            "src_port": random.randint(1024, 65535),
            "dst_port": random.randint(1, 10000),
            "protocol": 6 if random.random() > 0.3 else 17,
            "packets": 0,
            "bytes": 0,
            "flow_duration": 0
        }
        
        if not is_attack:
            # Normal traffic
            event["packets"] = random.randint(5, 50)
            event["bytes"] = random.randint(500, 5000)
            event["flow_duration"] = random.uniform(0.1, 5.0)
            event["dst_port"] = random.choice([80, 443, 8080, 53, 22])
        else:
            # Attack traffic
            if attack_type == "ddos":
                event["packets"] = random.randint(1000, 10000)
                event["bytes"] = random.randint(50000, 10000000)
                event["flow_duration"] = random.uniform(0.1, 2.0)
            elif attack_type == "port_scan":
                event["packets"] = random.randint(1, 3)
                event["bytes"] = random.randint(64, 200)
                event["dst_port"] = random.randint(1, 65535)
                event["src_ip"] = "192.168.1.100" # Single source
            else:
                event["packets"] = random.randint(500, 2000)
                event["bytes"] = random.randint(10000, 100000)
                event["flow_duration"] = random.uniform(1.0, 10.0)
        
        events.append(event)
    
    return events

def train_simulation_model():
    logger.info("Generating synthetic training data...")
    
    # Generate data
    normal_events = generate_traffic(5000, is_attack=False)
    ddos_events = generate_traffic(1000, is_attack=True, attack_type="ddos")
    scan_events = generate_traffic(1000, is_attack=True, attack_type="port_scan")
    brute_events = generate_traffic(1000, is_attack=True, attack_type="brute_force")
    
    all_events = normal_events + ddos_events + scan_events + brute_events
    labels = [0] * len(normal_events) + [1] * (len(ddos_events) + len(scan_events) + len(brute_events))
    
    df = pd.DataFrame(all_events)
    df['connection_id'] = range(len(df)) # Ensure unique IDs
    
    # Feature Engineering
    logger.info("Extracting features...")
    config = {'features': {'network': True, 'temporal': True}}
    fe = FeatureEngineer(config)
    
    # Initialize scaler by fitting on partial data
    # (Actually FeatureEngineer converts to dataframe, then we normalize)
    features = fe.extract_all_features(df)
    
    # Train/Test Split
    X = features.values
    y = np.array(labels)
    
    # Normalize
    # We must fit the scaler here and save it!
    logger.info("Fitting scaler...")
    X_scaled = fe.scaler.fit_transform(X)
    
    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)
    
    # Train Model
    logger.info("Training Gradient Boosting model...")
    model = GradientBoostingClassifier(n_estimators=100, random_state=42)
    model.fit(X_train, y_train)
    
    # Evaluate
    y_pred = model.predict(X_test)
    logger.info(f"Accuracy: {accuracy_score(y_test, y_pred):.4f}")
    logger.info(f"F1 Score: {f1_score(y_test, y_pred):.4f}")
    
    # Save Artifacts
    output_dir = "data/models"
    os.makedirs(os.path.join(output_dir, "anomaly_detector"), exist_ok=True)
    
    # Save model (wrapped in dict for app loader? No, repackage_models.py handles that)
    # But wait, app.py expects a specific dictionary format OR we use repackage_models.py again.
    # Let's write it in the final format directly.
    
    model_package = {
        'model': model,
        'scaler': fe.scaler,
        'config': {
            'features': config['features'],
            'model_type': 'simulation_gradient_boosting'
        }
    }
    
    model_path = os.path.join(output_dir, "anomaly_detector", "isolation_forest.pkl")
    joblib.dump(model_package, model_path)
    logger.info(f"Saved model package to {model_path}")
    
    # Save scaler separately as well (for good measure / benchmark scripts)
    joblib.dump(fe.scaler, os.path.join(output_dir, "scaler.pkl"))
    
    # Save feature_engineer state
    fe.save(os.path.join(output_dir, "feature_engineer.pkl"))
    logger.info("Saved feature engineer state")

if __name__ == "__main__":
    train_simulation_model()
